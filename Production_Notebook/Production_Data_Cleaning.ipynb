{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import cm as cmaps\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter, OrderedDict\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "print(\"Packages loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hk_df=load_original_data() \n",
    "hk_df=lower_case_column_names(hk_df)\n",
    "##hk_df=rename_columns(hk_df)\n",
    "#hk_df=drop_columns(hk_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hk_df=load_original_data() \n",
    "\n",
    "hk_df= (hk_df.pipe(lower_case_column_names)\\\n",
    ".pipe(rename_columns)\n",
    " #.pipe(drop_columns)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ DATA INTO DATAFRAME FROM EXISTING .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_data():\n",
    "    \"\"\"\n",
    "    Read production data and parse into pandas dataframe.\n",
    "    \"\"\"\n",
    "    latest_file = r\"C:\\Users\\p.kollhof\\Documents\\IRONHACK_GitHub\\DA_Midterm_Project\\Data\\Production_Data.csv\"\n",
    "    df_prod_whole = pd.read_csv(latest_file, sep=\";\", encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    return df_prod_whole\n",
    "\n",
    "df_prod_whole = load_original_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete unncessary/empty colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_unncessary_columns(df):\n",
    "    \"\"\"\n",
    "    Delete columns only containing NaN values or columns with effectively useless data\n",
    "    \"\"\"\n",
    "    #try:\n",
    "    cols_to_drop = []\n",
    "    if any(df_prod_whole.columns.str.contains(\"WT1\")):\n",
    "        cols_to_drop_1 = [col for col in df_prod_whole.columns if \"WT1\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_1)\n",
    "    if any(df_prod_whole.columns.str.contains(\"WT2\")):\n",
    "        cols_to_drop_2 = [col for col in df_prod_whole.columns if \"WT2\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_2)\n",
    "    if any(df_prod_whole.columns.str.contains(\"ET2\")):\n",
    "        cols_to_drop_3 = [col for col in df_prod_whole.columns if \"ET2\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_3)\n",
    "    if any(df_prod_whole.columns.str.contains(\"FT2\")):\n",
    "        cols_to_drop_4 = [col for col in df_prod_whole.columns if \"FT2\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_4)\n",
    "    if any(df_prod_whole.columns.str.contains(\"Schneiden_Ort\")):\n",
    "        cols_to_drop_5 = [col for col in df_prod_whole.columns if \"Schneiden_Ort\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_5)\n",
    "    if any(df_prod_whole.columns.str.contains(\"Schneiden_Geraet\")):\n",
    "        cols_to_drop_6 = [col for col in df_prod_whole.columns if \"Schneiden_Geraet\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_6)\n",
    "    if any(df_prod_whole.columns.str.contains(\"Schleifen_Ort\")):\n",
    "        cols_to_drop_7 = [col for col in df_prod_whole.columns if \"Schleifen_Ort\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_7)\n",
    "    if any(df_prod_whole.columns.str.contains(\"Schleifen_Geraet\")):\n",
    "        cols_to_drop_8 = [col for col in df_prod_whole.columns if \"Schleifen_Geraet\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_8)\n",
    "    if any(df_prod_whole.columns.str.contains(\"NS2_Ort\")):\n",
    "        cols_to_drop_9 = [col for col in df_prod_whole.columns if \"NS2_Ort\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_9)\n",
    "    if any(df_prod_whole.columns.str.contains(\"OeO_Ort\")):\n",
    "        cols_to_drop_10 = [col for col in df_prod_whole.columns if \"OeO_Ort\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_10)\n",
    "    if any(df_prod_whole.columns.str.contains(\"OeO_Geraet\")):\n",
    "        cols_to_drop_11 = [col for col in df_prod_whole.columns if \"OeO_Geraet\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_11)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Columns have already been deleted!\")\n",
    "\n",
    "    cols_to_drop.extend([\"Versand_Intern\", \"SSMA_TimeStamp\", \"Assembley_Teflonschlauch-Charge\"])\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    #except:\n",
    "        #raise ValueError(\"Deleting columns was unsuccessful!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_cleaned = delete_unncessary_columns(df_prod_whole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make columns lowercase and fix syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_columns_lowercase(df):\n",
    "    \n",
    "    lower_cols = [i.lower().replace(\" \", \"_\").replace(\"-\", \"_\") for i in df.columns]\n",
    "    df_cleaned.columns = lower_cols    \n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "df_cleaned = make_columns_lowercase(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows with XDC-ID < `68.000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_with_XDC_ID_pre_68k(df):\n",
    "    \"\"\"\n",
    "    Delete all entries before XDC-ID 68.000 due to them having wrong defect IDs.\n",
    "    Only applies if dataframe actually has older entries with XDC-IDs < 68.000\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df[df[\"pdc_nummer\"]>=68000]\n",
    "    \n",
    "    return df\n",
    "\n",
    "#df_cleaned = delete_rows_with_XDC_ID_pre_68k(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows with non-legit XDC-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_non_legit_XDC_IDs(df):\n",
    "    \"\"\"\n",
    "    Only keep entries with legit XDC-ID.\n",
    "    Anything before 44145 is not usable.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df[df[\"pdc_nummer\"]>=44145]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_cleaned = delete_non_legit_XDC_IDs(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast `string-dates` into `datetime-dates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_columns_transform_into_datetime(df):\n",
    "    \"\"\"\n",
    "    Transform all dates of string type 'YYYY-MM-DD' into dt.datetime format.\n",
    "    \"\"\"\n",
    "    date_cols = [col for col in df.columns if \"datum\" in col]\n",
    "\n",
    "    for col in date_cols:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df[col] = [dt.datetime.strptime(d,'%Y-%m-%d').date() if type(d)==str \n",
    "                       else np.nan if type(d)==float\n",
    "                       else d\n",
    "                       for d in df[col]]\n",
    "            \n",
    "    return df, date_cols\n",
    "\n",
    "df_cleaned, date_columns = date_columns_transform_into_datetime(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows without entries in selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_with_nan_in_selected_cols(df, cols):\n",
    "    \"\"\"\n",
    "    Delete rows that do not have an entry in the given columns\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        df[col].dropna(inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    return df\n",
    "    \n",
    "df_cleaned = delete_rows_with_nan_in_selected_cols(df_cleaned, [\"schmelzen_datum\", \"ns1_datum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substitute Product-`Familie` int-values with strings *(\"PDC\", \"PDC-C\", \"XDC\")*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_familie_int_str(df):\n",
    "    \"\"\"\n",
    "    Parse int values (1,2,3) into string values (\"pdc\", \"pdc-c\", \"xdc\")\n",
    "    \"\"\"\n",
    "    \n",
    "    if df[\"familie\"].dtype == \"int64\":\n",
    "        df[\"familie\"] = [\"pdc\" if fam==1\n",
    "                         else \"pdc-c\" if fam==2\n",
    "                         else \"xdc\" \n",
    "                         for fam in df_cleaned[\"familie\"]]\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_cleaned = substitute_familie_int_str(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `_Months` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_month_string_columns(df, date_columns):\n",
    "    \"\"\"\n",
    "    Convert YYYY-MM-DD dt.datetime into new 'YYYY-MM' string month column. \n",
    "    \"\"\"\n",
    "    \n",
    "    unwanted_dates = ['ts_in_datum', 'ts_out_datum', 'rt_bqc_in_datum', 'rt_bqc_out_datum']\n",
    "    date_columns_wanted = [col for col in date_columns if col not in unwanted_dates]\n",
    "    date_columns_months = [col.split(\"_\")[0] + \"_month\" for col in date_columns_wanted]\n",
    "    #date_columns_months = [col  for col in date_columns_months]\n",
    "    \n",
    "    #if \"schmelzen_Mmnths\" in df.columns:\n",
    "    for idx, datum in enumerate(date_columns_wanted):\n",
    "        df[date_columns_months[idx]] = [str(d)[:-3] for d in df[datum]]\n",
    "    #else:\n",
    "        #raise ValueError(\"Months columns have already been added!\")\n",
    "    return df\n",
    "\n",
    "df_cleaned = add_date_month_string_columns(df_cleaned, date_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn string-comma-values into dot-float-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_comma_to_dot_float(df):\n",
    "    \"\"\"\n",
    "    Cast string-comma-values as dot-float-values\n",
    "    \"\"\"\n",
    "    cols_to_use = ['ns1_winkel_vorne','ns1_winkel_hinten','ns1_durchmesser_vorne','ns1_durchmesser_hinten',\n",
    "               \"ns2_center_variation\", \"ns2_front_angle\",\"ns2_curve\",\"ns2_15xlength\",\n",
    "               \"ns2_parabolic_factor\",\"oeo_oeo\", \"piezo_phase\"] \n",
    "\n",
    "    for column in cols_to_use:\n",
    "        df[column] = [float(str(x).replace(',','.')) for x in df[column]] \n",
    "     \n",
    "    return df\n",
    "\n",
    "df_cleaned = string_comma_to_dot_float(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean personal (operator) initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_personal_names(df):\n",
    "    \"\"\"\n",
    "    Func descr\n",
    "    \"\"\"\n",
    "    \n",
    "    replacements = {'GD': 'GDH',\n",
    "                    'MFW': 'MFE',\n",
    "                    \"ME\": \"MFE\",\n",
    "                    \"MF\": \"MFE\",\n",
    "                    \"LAE\": \"AL\",\n",
    "                    \"SF\": \"SFZ\",\n",
    "                    \"DC\": \"AC\",\n",
    "                    \"THANH\": \"TT\",\n",
    "                    \"?\": np.nan}\n",
    "    \n",
    "    personal_cols = [col for col in df_cleaned.columns if \"personal\" in col]\n",
    "    \n",
    "    for col in personal_cols:\n",
    "        op_list = []\n",
    "\n",
    "        for op in df_cleaned[\"ft1_personal\"]:\n",
    "            if type(op)==str and op.isalpha():\n",
    "                op_list.append(op.upper())\n",
    "            elif op==\"?\":\n",
    "                op_list.append(np.nan)\n",
    "            elif type(op)==str and \".\" in op:\n",
    "                op_list.append(np.nan)\n",
    "            elif type(op)==str and not op.isalpha() and op!=\"?\":\n",
    "                op_list.append(op[:re.search(r'\\W+', s).start()].upper())\n",
    "            else:\n",
    "                op_list.append(np.nan)\n",
    "\n",
    "        op_list = [replacements.get(word, word) for word in op_list]\n",
    "\n",
    "        df[col] = op_list\n",
    "        \n",
    "    return df\n",
    "    \n",
    "df_cleaned = clean_personal_names(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
