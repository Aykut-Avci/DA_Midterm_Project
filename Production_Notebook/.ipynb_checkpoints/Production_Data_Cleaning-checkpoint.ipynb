{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import cm as cmaps\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter, OrderedDict\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "print(\"Packages loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hk_df=load_original_data() \n",
    "hk_df=lower_case_column_names(hk_df)\n",
    "##hk_df=rename_columns(hk_df)\n",
    "#hk_df=drop_columns(hk_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hk_df=load_original_data() \n",
    "\n",
    "hk_df= (hk_df.pipe(lower_case_column_names)\\\n",
    ".pipe(rename_columns)\n",
    " #.pipe(drop_columns)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ DATA INTO DATAFRAME FROM EXISTING .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_data():\n",
    "    \"\"\"\n",
    "    Read production data and parse into pandas dataframe.\n",
    "    \"\"\"\n",
    "    latest_file = r\"C:\\Users\\p.kollhof\\Documents\\IRONHACK_GitHub\\DA_Midterm_Project\\Data\\Production_Data.csv\"\n",
    "    df_prod_whole = pd.read_csv(latest_file, sep=\";\", encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    return df_prod_whole\n",
    "\n",
    "df_prod_whole = load_original_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete unncessary/empty colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True,  True,  True])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prod_whole.columns.str.contains(\"WT1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_unncessary_columns(df):\n",
    "    \"\"\"\n",
    "    Delete columns only containing NaN values or columns with effectively useless data\n",
    "    \"\"\"\n",
    "    #try:\n",
    "    cols_to_drop = []\n",
    "    if any(df_prod_whole.columns.str.contains(\"WT1\")):\n",
    "        cols_to_drop_1 = [col for col in df_prod_whole.columns if \"WT1\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_1)\n",
    "    if any(df_prod_whole.columns.str.contains(\"WT2\")):\n",
    "        cols_to_drop_2 = [col for col in df_prod_whole.columns if \"WT2\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_2)\n",
    "    if any(df_prod_whole.columns.str.contains(\"ET2\")):\n",
    "        cols_to_drop_3 = [col for col in df_prod_whole.columns if \"ET2\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_3)\n",
    "    if any(df_prod_whole.columns.str.contains(\"FT2\")):\n",
    "        cols_to_drop_4 = [col for col in df_prod_whole.columns if \"FT2\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_4)\n",
    "    if any(df_prod_whole.columns.str.contains(\"Schneiden_Ort\")):\n",
    "        cols_to_drop_5 = [col for col in df_prod_whole.columns if \"Schneiden_Ort\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_5)\n",
    "    if any(df_prod_whole.columns.str.contains(\"Schneiden_Geraet\")):\n",
    "        cols_to_drop_6 = [col for col in df_prod_whole.columns if \"Schneiden_Geraet\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_6)\n",
    "    if any(df_prod_whole.columns.str.contains(\"Schleifen_Ort\")):\n",
    "        cols_to_drop_7 = [col for col in df_prod_whole.columns if \"Schleifen_Ort\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_7)\n",
    "    if any(df_prod_whole.columns.str.contains(\"Schleifen_Geraet\")):\n",
    "        cols_to_drop_8 = [col for col in df_prod_whole.columns if \"Schleifen_Geraet\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_8)\n",
    "    if any(df_prod_whole.columns.str.contains(\"NS2_Ort\")):\n",
    "        cols_to_drop_9 = [col for col in df_prod_whole.columns if \"NS2_Ort\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_9)\n",
    "    if any(df_prod_whole.columns.str.contains(\"OeO_Ort\")):\n",
    "        cols_to_drop_10 = [col for col in df_prod_whole.columns if \"OeO_Ort\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_10)\n",
    "    if any(df_prod_whole.columns.str.contains(\"OeO_Geraet\")):\n",
    "        cols_to_drop_11 = [col for col in df_prod_whole.columns if \"OeO_Geraet\" in col]\n",
    "        cols_to_drop.extend(cols_to_drop_11)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Columns have already been deleted!\")\n",
    "\n",
    "    cols_to_drop.extend([\"Versand_Intern\", \"SSMA_TimeStamp\", \"Assembley_Teflonschlauch-Charge\"])\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    #except:\n",
    "        #raise ValueError(\"Deleting columns was unsuccessful!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_cleaned = delete_unncessary_columns(df_prod_whole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make columns lowercase and fix syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_columns_lowercase(df):\n",
    "    \n",
    "    lower_cols = [i.lower().replace(\" \", \"_\").replace(\"-\", \"_\") for i in df.columns]\n",
    "    df_cleaned.columns = lower_cols    \n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "df_cleaned = make_columns_lowercase(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows with XDC-ID < `68.000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_with_XDC_ID_pre_68k(df):\n",
    "    \"\"\"\n",
    "    Delete all entries before XDC-ID 68.000 due to them having wrong defect IDs.\n",
    "    Only applies if dataframe actually has older entries with XDC-IDs < 68.000\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df[df[\"pdc_nummer\"]>=68000]\n",
    "    \n",
    "    return df\n",
    "\n",
    "#df_cleaned = delete_rows_with_XDC_ID_pre_68k(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows with non-legit XDC-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_non_legit_XDC_IDs(df):\n",
    "    \"\"\"\n",
    "    Only keep entries with legit XDC-ID.\n",
    "    Anything before 44145 is not usable.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df[df[\"pdc_nummer\"]>=44145]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_cleaned = delete_non_legit_XDC_IDs(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast `string-dates` into `datetime-dates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_columns_transform_into_datetime(df):\n",
    "    \"\"\"\n",
    "    Transform all dates of string type 'YYYY-MM-DD' into dt.datetime format.\n",
    "    \"\"\"\n",
    "    date_cols = [col for col in df.columns if \"datum\" in col]\n",
    "\n",
    "    for col in date_cols:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df[col] = [dt.datetime.strptime(d,'%Y-%m-%d').date() if type(d)==str \n",
    "                       else np.nan if type(d)==float\n",
    "                       else d\n",
    "                       for d in df[col]]\n",
    "            \n",
    "    return df, date_cols\n",
    "\n",
    "df_cleaned, date_columns = date_columns_transform_into_datetime(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows without entries in selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_with_nan_in_selected_cols(df, cols):\n",
    "    \"\"\"\n",
    "    Delete rows that do not have an entry in the given columns\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        df[col].dropna(inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    return df\n",
    "    \n",
    "df_cleaned = delete_rows_with_nan_in_selected_cols(df_cleaned, [\"schmelzen_datum\", \"ns1_datum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substitute Product-`Familie` int-values with strings *(\"PDC\", \"PDC-C\", \"XDC\")*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_familie_int_str(df):\n",
    "    \"\"\"\n",
    "    Parse int values (1,2,3) into string values (\"pdc\", \"pdc-c\", \"xdc\")\n",
    "    \"\"\"\n",
    "    \n",
    "    if df[\"familie\"].dtype == \"int64\":\n",
    "        df[\"familie\"] = [\"pdc\" if fam==1\n",
    "                         else \"pdc-c\" if fam==2\n",
    "                         else \"xdc\" \n",
    "                         for fam in df_cleaned[\"familie\"]]\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_cleaned = substitute_familie_int_str(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `_Months` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_month_string_columns(df, date_columns):\n",
    "    \"\"\"\n",
    "    Convert YYYY-MM-DD dt.datetime into new 'YYYY-MM' string month column. \n",
    "    \"\"\"\n",
    "    \n",
    "    unwanted_dates = ['ts_in_datum', 'ts_out_datum', 'rt_bqc_in_datum', 'rt_bqc_out_datum']\n",
    "    date_columns_wanted = [col for col in date_columns if col not in unwanted_dates]\n",
    "    date_columns_months = [col.split(\"_\")[0] + \"_month\" for col in date_columns_wanted]\n",
    "    #date_columns_months = [col  for col in date_columns_months]\n",
    "    \n",
    "    #if \"schmelzen_Mmnths\" in df.columns:\n",
    "    for idx, datum in enumerate(date_columns_wanted):\n",
    "        df[date_columns_months[idx]] = [str(d)[:-3] for d in df[datum]]\n",
    "    #else:\n",
    "        #raise ValueError(\"Months columns have already been added!\")\n",
    "    return df\n",
    "\n",
    "df_cleaned = add_date_month_string_columns(df_cleaned, date_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> To Fix - Aykut\n",
    "\n",
    "- turn comma string float values (e.g. \"12,3\") into dot float values (12.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_personal_names(df):\n",
    "    \"\"\"\n",
    "    Func descr\n",
    "    \"\"\"\n",
    "    \n",
    "    replacements = {'GD': 'GDH',\n",
    "                    'MFW': 'MFE',\n",
    "                    \"ME\": \"MFE\",\n",
    "                    \"MF\": \"MFE\",\n",
    "                    \"LAE\": \"AL\",\n",
    "                    \"SF\": \"SFZ\",\n",
    "                    \"DC\": \"AC\",\n",
    "                    \"?\": np.nan}\n",
    "    \n",
    "    personal_cols = [col for col in df_cleaned.columns if \"personal\" in col]\n",
    "    for col in personal_cols:\n",
    "        pers_col = []\n",
    "        \n",
    "        for op in df[col]:\n",
    "            if type(op)==float or op==\"?\":\n",
    "                pers_col.append(np.nan)\n",
    "            if type(op)==str and op.isalpha():\n",
    "                pers_col.append(op.split(\",\")[0].upper())\n",
    "\n",
    "        pers_col = [replacements.get(word, word) for word in pers_col]\n",
    "\n",
    "        df[col] = pers_col\n",
    "        \n",
    "        return df\n",
    "    \n",
    "df_cleaned = clean_personal_names(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'SC', 'MB', 'MS', 'FH', 'RC', 'MFe', 'AL', 'FF', 'Mfe', 'AC',\n",
       "       'AT', 'LE', '?', 'fh', 'SC, BR', 'GD', 'sc', 'SFZ', 'Le', 'le',\n",
       "       'MF', 'GDH', 'DC', 'ff', 'MFE', 'FF, BR', 'BR', 'PE', 'WB', 'EC',\n",
       "       'KrM', 'pe', 'PK', 'SF', 'LA', 'MFe, BR', 'KR', 'MFw', 'Kr',\n",
       "       '28.6.19', 'ME'], dtype=object)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[\"ft1_personal\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 47 entries, 8714 to 48464\n",
      "Columns: 146 entries, pdc_nummer to versand_month\n",
      "dtypes: float64(47), int64(4), object(95)\n",
      "memory usage: 54.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned[df_cleaned[\"ft1_personal\"]==\"?\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to parse string \"8,8\" at position 423",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2315\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \"8,8\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNS1_Winkel_vorne\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNS1_Winkel_vorne\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNS1_Winkel_hinten\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mto_numeric(df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNS1_Winkel_hinten\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNS2_Center_Variation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mto_numeric(df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNS2_Center_Variation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\tools\\numeric.py:184\u001b[0m, in \u001b[0;36mto_numeric\u001b[1;34m(arg, errors, downcast)\u001b[0m\n\u001b[0;32m    182\u001b[0m coerce_numeric \u001b[38;5;241m=\u001b[39m errors \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m     values, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_numeric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoerce_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_numeric\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2357\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \"8,8\" at position 423"
     ]
    }
   ],
   "source": [
    "#df_cleaned['NS1_Winkel_vorne']=pd.to_numeric(df_cleaned['NS1_Winkel_vorne'])\n",
    "#df_cleaned['NS1_Winkel_hinten']=pd.to_numeric(df_cleaned['NS1_Winkel_hinten'])\n",
    "\n",
    "#df_cleaned['NS2_Center_Variation']=pd.to_numeric(df_cleaned['NS2_Center_Variation'])\n",
    "#df_cleaned['NS2_Front_Angle']=pd.to_numeric(df_cleaned['NS2_Front_Angle'])\n",
    "#df_cleaned['NS2_Curve']=pd.to_numeric(df_cleaned['NS2_Curve'])\n",
    "#df_cleaned['NS2_15xLength']=pd.to_numeric(df_cleaned['NS2_15xLength'])\n",
    "#df_cleaned['NS2_Parabolic_Factor']=pd.to_numeric(df_cleaned['NS2_Parabolic_Factor'])\n",
    "\n",
    "#df_cleaned['OeO_OeO']=pd.to_numeric(df_cleaned['OeO_OeO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##############################################\n",
    "### FOLDER & PATH CREATION FOR DATA SAVING ###\n",
    "\n",
    "### CREATE FOLDERS\n",
    "Subfolder_Names = [\"Scraps\", \"Bonding_Assembly\", \n",
    "                   \"OP_ThruPut_General\", \"OP_ThruPut_Melting\", \"MD_ThruPut_Melting\", \"FH\"]\n",
    "try:\n",
    "    os.makedirs('Data_created_' + dt.datetime.now().strftime(\"%y-%m-%d\") + \"_PROD\" + \"\\\\\")\n",
    "    for subfolder in Subfolder_Names:\n",
    "        os.makedirs('Data_created_' + dt.datetime.now().strftime(\"%y-%m-%d\") + \"_PROD\" + \"\\\\\" + subfolder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "### CREATE PATH\n",
    "savePath = os.getcwd() + \"\\\\\" + 'Data_created_' + dt.datetime.now().strftime(\"%y-%m-%d\") + \"_PROD\" + \"\\\\\"\n",
    "savePath_Scrap = os.getcwd() + \"\\\\\" + 'Data_created_' + dt.datetime.now().strftime(\"%y-%m-%d\") + \"_PROD\" + \"\\\\\" + \"Scraps\" + \"\\\\\"\n",
    "savePath_BondAss = os.getcwd() + \"\\\\\" + 'Data_created_' + dt.datetime.now().strftime(\"%y-%m-%d\") + \"_PROD\" + \"\\\\\" + \"Bonding_Assembly\" + \"\\\\\"\n",
    "savePath_OP_TP_General = os.getcwd() + \"\\\\\" + 'Data_created_' + dt.datetime.now().strftime(\"%y-%m-%d\") + \"_PROD\" + \"\\\\\" + \"OP_ThruPut_General\" + \"\\\\\"\n",
    "savePath_OP_TP_Melt = os.getcwd() + \"\\\\\" + 'Data_created_' + dt.datetime.now().strftime(\"%y-%m-%d\") + \"_PROD\" + \"\\\\\" + \"OP_ThruPut_Melting\" + \"\\\\\"\n",
    "savePath_MD_TP_Melt = os.getcwd() + \"\\\\\" + 'Data_created_' + dt.datetime.now().strftime(\"%y-%m-%d\") + \"_PROD\" + \"\\\\\" + \"MD_ThruPut_Melting\" + \"\\\\\"\n",
    "\n",
    "savePath_FH = os.getcwd() + \"\\\\\" + 'Data_created_' + dt.datetime.now().strftime(\"%y-%m-%d\") + \"_PROD\" + \"\\\\\" + \"FH5\" + \"\\\\\"\n",
    "\n",
    "### FOLDER & PATH CREATION FOR DATA SAVING ###\n",
    "##############################################\n",
    "\n",
    "\n",
    "### COLOR SCHEMES FOR PLOTS\n",
    "cmap_PDC_Sizes = [\"grey\", \"gold\", \"orange\", \"red\", \"green\", \"mediumblue\", \"darkviolet\"]\n",
    "cmap_Coating_Types = [\"grey\", \"gold\", \"red\", \"green\", \"mediumblue\"]\n",
    "cmap_fQC_Types = [\"grey\", \"silver\", \"red\", \"green\", \"mediumblue\"]\n",
    "cmap_QC_Types = [\"grey\", \"red\", \"green\", \"mediumblue\"]\n",
    "cmap_OPs = [\"orangered\", \"forestgreen\", \"deepskyblue\", \"orange\", \"cyan\", \"fuchsia\", \"dimgrey\"]\n",
    "\n",
    "###########################################\n",
    "### PROCESS STEP DATA HANDLING - MONTHS ###\n",
    "\n",
    "\n",
    "### UPDATE DATAFRAME OPERATORS WITH CAPITALIZATION & SINGLE-OP ###\n",
    "Schmelzen_OP_List = list(df_DB_trunc[\"Schmelzen_Personal\"])\n",
    "Schneiden_OP_List = list(df_DB_trunc[\"Schneiden_Personal\"])\n",
    "NS1_OP_List = list(df_DB_trunc[\"NS1_Personal\"])\n",
    "Schleifen_OP_List = list(df_DB_trunc[\"Schleifen_Personal\"])\n",
    "NS2_OP_List = list(df_DB_trunc[\"NS2_Personal\"])\n",
    "OeO_OP_List = list(df_DB_trunc[\"OeO_Personal\"])\n",
    "K1_OP_List = list(df_DB_trunc[\"Kleben1_Personal\"])\n",
    "K2_OP_List = list(df_DB_trunc[\"Kleben2_Personal\"])\n",
    "Ass_OP_List = list(df_DB_trunc[\"Assembley_Personal\"])\n",
    "BQC_OP_List = list(df_DB_trunc[\"BQC_Personal\"])\n",
    "ET_OP_List = list(df_DB_trunc[\"ET1_Personal\"])\n",
    "FT_OP_List = list(df_DB_trunc[\"FT1_Personal\"])\n",
    "EK_OP_List = list(df_DB_trunc[\"EK_Personal\"])\n",
    "\n",
    "for schmop in range(len(Schmelzen_OP_List)):\n",
    "    if Schmelzen_OP_List[schmop] == 0 or type(Schmelzen_OP_List[schmop])==float:\n",
    "        pass\n",
    "    elif Schmelzen_OP_List[schmop].isupper():\n",
    "        Schmelzen_OP_List[schmop] = Schmelzen_OP_List[schmop][:2]\n",
    "    elif not Schmelzen_OP_List[schmop].isupper():\n",
    "        Schmelzen_OP_List[schmop] = Schmelzen_OP_List[schmop][:2].upper()\n",
    "\n",
    "for schnop in range(len(Schneiden_OP_List)):\n",
    "    if Schneiden_OP_List[schnop] == 0 or type(Schneiden_OP_List[schnop])==float:\n",
    "        pass\n",
    "    elif Schneiden_OP_List[schnop].isupper():\n",
    "        Schneiden_OP_List[schnop] = Schneiden_OP_List[schnop][:2]\n",
    "    elif not Schneiden_OP_List[schnop].isupper():\n",
    "        Schneiden_OP_List[schnop] = Schneiden_OP_List[schnop][:2].upper()\n",
    "        \n",
    "for ns1op in range(len(NS1_OP_List)):\n",
    "    if NS1_OP_List[ns1op] == 0 or type(NS1_OP_List[ns1op])==float:\n",
    "        pass\n",
    "    elif NS1_OP_List[ns1op].isupper():\n",
    "        NS1_OP_List[ns1op] = NS1_OP_List[ns1op][:2]\n",
    "    elif not NS1_OP_List[ns1op].isupper():\n",
    "        NS1_OP_List[ns1op] = NS1_OP_List[ns1op][:2].upper()\n",
    "\n",
    "for ns2op in range(len(NS2_OP_List)):\n",
    "    if NS2_OP_List[ns2op] == 0 or type(NS2_OP_List[ns2op])==float:\n",
    "        pass\n",
    "    elif NS2_OP_List[ns2op].isupper():\n",
    "        NS2_OP_List[ns2op] = NS2_OP_List[ns2op][:2]\n",
    "    elif not NS2_OP_List[ns2op].isupper():\n",
    "        NS2_OP_List[ns2op] = NS2_OP_List[ns2op][:2].upper()\n",
    "\n",
    "for oeoop in range(len(OeO_OP_List)):\n",
    "    if OeO_OP_List[oeoop] == 0 or type(OeO_OP_List[oeoop])==float:\n",
    "        pass\n",
    "    elif OeO_OP_List[oeoop].isupper():\n",
    "        OeO_OP_List[oeoop] = OeO_OP_List[oeoop][:2]\n",
    "    elif not OeO_OP_List[oeoop].isupper():\n",
    "        OeO_OP_List[oeoop] = OeO_OP_List[oeoop][:2].upper()\n",
    "\n",
    "for k1op in range(len(K1_OP_List)):\n",
    "    if K1_OP_List[k1op] == 0 or type(K1_OP_List[k1op])==float:\n",
    "        pass\n",
    "    elif K1_OP_List[k1op].isupper():\n",
    "        K1_OP_List[k1op] = K1_OP_List[k1op][:2]\n",
    "    elif not K1_OP_List[k1op].isupper():\n",
    "        K1_OP_List[k1op] = K1_OP_List[k1op][:2].upper()\n",
    "        \n",
    "for k2op in range(len(K2_OP_List)):\n",
    "    if K2_OP_List[k2op] == 0 or type(K2_OP_List[k2op])==float:\n",
    "        pass\n",
    "    elif K2_OP_List[k2op].isupper():\n",
    "        K2_OP_List[k2op] = K2_OP_List[k2op][:2]\n",
    "    elif not K2_OP_List[k2op].isupper():\n",
    "        K2_OP_List[k2op] = K2_OP_List[k2op][:2].upper()\n",
    "        \n",
    "for assop in range(len(Ass_OP_List)):\n",
    "    if Ass_OP_List[assop] == 0 or type(Ass_OP_List[assop])==float:\n",
    "        pass\n",
    "    elif Ass_OP_List[assop].isupper():\n",
    "        Ass_OP_List[assop] = Ass_OP_List[assop][:2]\n",
    "    elif not Ass_OP_List[assop].isupper():\n",
    "        Ass_OP_List[assop] = Ass_OP_List[assop][:2].upper()\n",
    "        \n",
    "for bqcop in range(len(BQC_OP_List)):\n",
    "    if BQC_OP_List[bqcop] == 0 or type(BQC_OP_List[bqcop])==float:\n",
    "        pass\n",
    "    elif BQC_OP_List[bqcop].isupper():\n",
    "        BQC_OP_List[bqcop] = BQC_OP_List[bqcop][:3]\n",
    "    elif not BQC_OP_List[bqcop].isupper():\n",
    "        BQC_OP_List[bqcop] = BQC_OP_List[bqcop][:3].upper()\n",
    "        \n",
    "for etop in range(len(ET_OP_List)):\n",
    "    if ET_OP_List[etop] == \"MF\":\n",
    "        ET_OP_List[etop]==\"MFE\"\n",
    "    if ET_OP_List[etop] == 0 or type(ET_OP_List[etop])==float:\n",
    "        pass\n",
    "    elif ET_OP_List[etop].isupper():\n",
    "        ET_OP_List[etop] = ET_OP_List[etop][:3]\n",
    "    elif not ET_OP_List[etop].isupper():\n",
    "        ET_OP_List[etop] = ET_OP_List[etop][:3].upper()\n",
    "    if type(ET_OP_List[etop])!=float:\n",
    "        ET_OP_List[etop] = re.sub(r'[^\\w]', '', ET_OP_List[etop])\n",
    "        ET_OP_List[etop] = ''.join(i for i in ET_OP_List[etop] if not i.isdigit())\n",
    "\n",
    "FT_OP_List_ = []\n",
    "for ftop in range(len(FT_OP_List)):\n",
    "    if FT_OP_List[ftop] == \"MF\":\n",
    "        FT_OP_List_.append(\"MFE\")\n",
    "    elif FT_OP_List[ftop] == \"SC_\":\n",
    "        FT_OP_List_.append(\"SC\")\n",
    "    elif FT_OP_List[ftop] == 0 or type(FT_OP_List[ftop])==float:\n",
    "        FT_OP_List_.append(np.nan)\n",
    "    elif FT_OP_List[ftop].isupper():\n",
    "        FT_OP_List_.append(FT_OP_List[ftop][:3])\n",
    "    elif not FT_OP_List[ftop].isupper():\n",
    "        FT_OP_List_.append(FT_OP_List[ftop][:3].upper())\n",
    "    elif type(FT_OP_List[ftop])!=float:\n",
    "        FT_OP_List_.append(re.sub(r'[^\\w]', '', FT_OP_List[ftop]))\n",
    "        FT_OP_List_.append(''.join(i for i in FT_OP_List[ftop] if not i.isdigit()))\n",
    "        \n",
    "for ekop in range(len(EK_OP_List)):\n",
    "    if EK_OP_List[ekop] == \"MF\":\n",
    "        EK_OP_List[ekop]==\"MFE\"\n",
    "    if EK_OP_List[ekop] == 0 or type(EK_OP_List[ekop])==float:\n",
    "        pass\n",
    "    elif EK_OP_List[ekop].isupper():\n",
    "        EK_OP_List[ekop] = EK_OP_List[ekop][:3]\n",
    "    elif not EK_OP_List[ekop].isupper():\n",
    "        EK_OP_List[ekop] = EK_OP_List[ekop][:3].upper()\n",
    "    if type(EK_OP_List[ekop])!=float:\n",
    "        EK_OP_List[ekop] = re.sub(r'[^\\w]', '', EK_OP_List[ekop])\n",
    "        EK_OP_List[ekop] = ''.join(i for i in EK_OP_List[ekop] if not i.isdigit())\n",
    "\n",
    "df_DB_trunc[\"Schmelzen_Personal\"] = Schmelzen_OP_List\n",
    "df_DB_trunc[\"Schneiden_Personal\"] = Schneiden_OP_List\n",
    "df_DB_trunc[\"NS1_Personal\"] = NS1_OP_List\n",
    "df_DB_trunc[\"NS2_Personal\"] = NS2_OP_List\n",
    "df_DB_trunc[\"OeO_Personal\"] = OeO_OP_List\n",
    "df_DB_trunc[\"Kleben1_Personal\"] = K1_OP_List\n",
    "df_DB_trunc[\"Kleben2_Personal\"] = K2_OP_List\n",
    "df_DB_trunc[\"Assembley_Personal\"] = Ass_OP_List\n",
    "df_DB_trunc[\"BQC_Personal\"] = BQC_OP_List\n",
    "df_DB_trunc[\"ET1_Personal\"] = ET_OP_List\n",
    "df_DB_trunc[\"FT1_Personal\"] = FT_OP_List\n",
    "df_DB_trunc[\"EK_Personal\"] = EK_OP_List\n",
    "\n",
    "### CREATE LISTS OF UNIQUE OPs FOR EACH PROCESS STEP ###\n",
    "Schmelzen_OP_List_Set = sorted([schmop for schmop in list(set(Schmelzen_OP_List)) if type(schmop)==str if len(schmop) > 1 if len(schmop) <= 3 if \"#\" not in schmop if \".\" not in schmop])\n",
    "Schneiden_OP_List_Set = sorted([schnop for schnop in list(set(Schneiden_OP_List)) if type(schnop)==str if len(schnop) > 1 if len(schnop) <= 3 if \"#\" not in schnop if \".\" not in schnop])\n",
    "NS1_OP_List_Set = sorted([ns1op for ns1op in list(set(NS1_OP_List)) if type(ns1op)==str if len(ns1op) > 1 if len(ns1op) <= 3 if \"#\" not in ns1op if \".\" not in ns1op])\n",
    "Schleifen_OP_List_Set = sorted([schlop for schlop in list(set(Schleifen_OP_List)) if type(schlop)==str if len(schlop) > 1 if len(schlop) <= 3 if \"#\" not in schlop if \".\" not in schlop])\n",
    "NS2_OP_List_Set = sorted([ns2op for ns2op in list(set(NS2_OP_List)) if type(ns2op)==str if len(ns2op) > 1 if len(ns2op) <= 3 if \"#\" not in ns2op if \".\" not in ns2op])\n",
    "OeO_OP_List_Set = sorted([oeoop for oeoop in list(set(OeO_OP_List)) if type(oeoop)==str if len(oeoop) > 1 if len(oeoop) <= 3 if \"#\" not in oeoop if \".\" not in oeoop])\n",
    "K1_OP_List_Set = sorted([k1op for k1op in list(set(K1_OP_List)) if type(k1op)==str if len(k1op) > 1 if len(k1op) <= 3 if \"#\" not in k1op if \".\" not in k1op])\n",
    "K2_OP_List_Set = sorted([k2op for k2op in list(set(K2_OP_List)) if type(k2op)==str if len(k2op) > 1 if len(k2op) <= 3 if \"#\" not in k2op if \".\" not in k2op])\n",
    "Ass_OP_List_Set = sorted([assop for assop in list(set(Ass_OP_List)) if type(assop)==str if len(assop) > 1 if len(assop) <= 3 if \"#\" not in assop if \".\" not in assop])\n",
    "BQC_OP_List_Set = sorted([bqcop for bqcop in list(set(BQC_OP_List)) if type(bqcop)==str if len(bqcop) > 1 if \"#\" not in bqcop if \".\" not in bqcop])\n",
    "ET_OP_List_Set = sorted([etop for etop in list(set(ET_OP_List)) if type(etop)==str if len(etop) > 1 if \"#\" not in etop if \".\" not in etop])\n",
    "FT_OP_List_Set = sorted([ftop for ftop in list(set(FT_OP_List_)) if type(ftop)==str if len(ftop) > 1 if \"#\" not in ftop if \".\" not in ftop if \"_\" not in ftop if \"?\" not in ftop])\n",
    "EK_OP_List_Set = sorted([ekop for ekop in list(set(EK_OP_List)) if type(ekop)==str if len(ekop) > 1 if \"#\" not in ekop if \".\" not in ekop])\n",
    "\n",
    "### CREATE LIST OF OVERALL UNIQUE OPERATORS\n",
    "OPs_All_List = [Schmelzen_OP_List_Set, Schneiden_OP_List_Set, NS1_OP_List_Set, \n",
    "                Schleifen_OP_List_Set, NS2_OP_List_Set, OeO_OP_List_Set, K1_OP_List_Set, \n",
    "                K2_OP_List_Set, Ass_OP_List_Set, BQC_OP_List_Set]\n",
    "OPs_All_List = sorted(list(set(list(itertools.chain(*OPs_All_List)))))\n",
    "\n",
    "\n",
    "\n",
    "### TRANSFORM STRING-COMMA-VALUES TO DOT-FLOAT-VALUES ###\n",
    "NS1VV_StrCommaVal_List = list(df_DB_trunc[\"NS1_Durchmesser_vorne\"])\n",
    "NS1VV_FloatDotVal_List = []\n",
    "for ns1vv in NS1VV_StrCommaVal_List:\n",
    "    if type(ns1vv) == float:\n",
    "        NS1VV_FloatDotVal_List.append(0)\n",
    "    elif type(ns1vv) == str:\n",
    "        ns1vv = ns1vv.replace(',', '.')\n",
    "        NS1VV_FloatDotVal_List.append(float(ns1vv))\n",
    "df_DB_trunc[\"NS1_Durchmesser_vorne\"] = NS1VV_FloatDotVal_List\n",
    "\n",
    "NS1VH_StrCommaVal_List = list(df_DB_trunc[\"NS1_Durchmesser_hinten\"])\n",
    "NS1VH_FloatDotVal_List = []\n",
    "for ns1vh in NS1VH_StrCommaVal_List:\n",
    "    if type(ns1vh) == float:\n",
    "        NS1VH_FloatDotVal_List.append(0)\n",
    "    elif type(ns1vh) == str:\n",
    "        ns1vh = ns1vh.replace(',', '.')\n",
    "        NS1VH_FloatDotVal_List.append(float(ns1vh))\n",
    "df_DB_trunc[\"NS1_Durchmesser_hinten\"] = NS1VH_FloatDotVal_List\n",
    "\n",
    "NS2PF_StrCommaVal_List = list(df_DB_trunc[\"NS2_Parabolic_Factor\"])\n",
    "NS2PF_FloatDotVal_List = []\n",
    "for ns2pf in NS2PF_StrCommaVal_List:\n",
    "    if type(ns2pf) == float:\n",
    "        NS2PF_FloatDotVal_List.append(0)\n",
    "    elif type(ns2pf) == str:\n",
    "        ns2pf = ns2pf.replace(',', '.')\n",
    "        NS2PF_FloatDotVal_List.append(float(ns2pf))\n",
    "df_DB_trunc[\"NS2_Parabolic_Factor\"] = NS2PF_FloatDotVal_List\n",
    "\n",
    "NS215x_StrCommaVal_List = list(df_DB_trunc[\"NS2_15xLength\"])\n",
    "NS215x_FloatDotVal_List = []\n",
    "for ns215x in NS215x_StrCommaVal_List:\n",
    "    if type(ns215x) == float:\n",
    "        NS215x_FloatDotVal_List.append(0)\n",
    "    elif type(ns215x) == str:\n",
    "        ns215x = ns215x.replace(',', '.')\n",
    "        NS215x_FloatDotVal_List.append(float(ns215x))\n",
    "df_DB_trunc[\"NS2_15xLength\"] = NS215x_FloatDotVal_List\n",
    "\n",
    "NS2CV_StrCommaVal_List = list(df_DB_trunc[\"NS2_Center_Variation\"])\n",
    "NS2CV_FloatDotVal_List = []\n",
    "for ns2cv in NS2CV_StrCommaVal_List:\n",
    "    if type(ns2cv) == float:\n",
    "        NS2CV_FloatDotVal_List.append(0)\n",
    "    elif type(ns2cv) == str:\n",
    "        ns2cv = ns2cv.replace(',', '.')\n",
    "        NS2CV_FloatDotVal_List.append(float(ns2cv))\n",
    "df_DB_trunc[\"NS2_Center_Variation\"] = NS2CV_FloatDotVal_List\n",
    "\n",
    "NS2FA_StrCommaVal_List = list(df_DB_trunc[\"NS2_Front_Angle\"])\n",
    "NS2FA_FloatDotVal_List = []\n",
    "for ns2fa in NS2FA_StrCommaVal_List:\n",
    "    if type(ns2fa) == float:\n",
    "        NS2FA_FloatDotVal_List.append(0)\n",
    "    elif type(ns2fa) == str:\n",
    "        ns2fa = ns2fa.replace(',', '.')\n",
    "        NS2FA_FloatDotVal_List.append(float(ns2fa))\n",
    "df_DB_trunc[\"NS2_Front_Angle\"] = NS2FA_FloatDotVal_List\n",
    "\n",
    "OeO_StrCommaVal_List = list(df_DB_trunc[\"OeO_OeO\"])\n",
    "OeO_FloatDotVal_List = []\n",
    "for oeo in OeO_StrCommaVal_List:\n",
    "    if type(oeo) == float:\n",
    "        OeO_FloatDotVal_List.append(0)\n",
    "    elif type(oeo) == str:\n",
    "        oeo = oeo.replace(',', '.')\n",
    "        OeO_FloatDotVal_List.append(float(oeo))\n",
    "df_DB_trunc[\"OeO_OeO\"] = OeO_FloatDotVal_List\n",
    "\n",
    "### SORT GENERAL DATA BY PRODUCTION SITE ###\n",
    "df_DB_Dortmund = df_DB_trunc[df_DB_trunc[\"Schmelzen_Ort\"]==\"Dortmund\"].reset_index(drop=True)\n",
    "df_DB_Berlin = df_DB_trunc[df_DB_trunc[\"Schmelzen_Ort\"]==\"Berlin\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Dataframe loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "die letzten X Monate:  7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2022-02', '2022-03', '2022-04', '2022-05', '2022-06', '2022-07']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DETERMINE TIMEFRAME TO PLOT ###\n",
    "LastNMonths = []\n",
    "TimeFrame = int(input(\"die letzten X Monate: \"))\n",
    "TimeFrame = list(range(0,TimeFrame,1))\n",
    "for l in TimeFrame:\n",
    "    last_month = dt.datetime.now() - relativedelta(months=l)\n",
    "    last_month_ = format(last_month, '%Y-%m')\n",
    "    LastNMonths.append(last_month_)\n",
    "    \n",
    "LastNMonths = LastNMonths[::-1]\n",
    "LastNMonths = LastNMonths[:-1]\n",
    "LastNMonths"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
